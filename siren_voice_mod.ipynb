{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ybW1vJ4BP334"
   },
   "source": [
    "# Siren Exploration\n",
    "\n",
    "This is a colab to explore properties of the Siren MLP, proposed in our work [Implicit Neural Activations with Periodic Activation Functions](https://vsitzmann.github.io/siren).\n",
    "\n",
    "\n",
    "We will first implement a streamlined version of Siren for fast experimentation. This lacks the code to easily do baseline comparisons - please refer to the main code for that - but will greatly simplify the code!\n",
    "\n",
    "**Make sure that you have enabled the GPU under Edit -> Notebook Settings!**\n",
    "\n",
    "We will then reproduce the following results from the paper: \n",
    "* [Fitting an image](#section_1)\n",
    "* [Fitting an audio signal](#section_2)\n",
    "* [Solving Poisson's equation](#section_3)\n",
    "* [Initialization scheme & distribution of activations](#activations)\n",
    "* [Distribution of activations is shift-invariant](#shift_invariance)\n",
    "\n",
    "We will also explore Siren's [behavior outside of the training range](#out_of_range).\n",
    "\n",
    "Let's go! First, some imports, and a function to quickly generate coordinate grids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hCqsVRCvP337",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import os\n",
    "\n",
    "from PIL import Image\n",
    "from torchvision.transforms import Resize, Compose, ToTensor, Normalize\n",
    "import numpy as np\n",
    "\n",
    "def get_mgrid(sidelen, dim=2):\n",
    "    '''Generates a flattened grid of (x,y,...) coordinates in a range of -1 to 1.\n",
    "    sidelen: int\n",
    "    dim: int'''\n",
    "    tensors = tuple(dim * [torch.linspace(-1, 1, steps=sidelen)])\n",
    "    mgrid = torch.stack(torch.meshgrid(*tensors, indexing=None), dim=-1)\n",
    "    mgrid = mgrid.reshape(-1, dim)\n",
    "    return mgrid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BX8e2eELP338"
   },
   "source": [
    "Now, we code up the sine layer, which will be the basic building block of SIREN. This is a much more concise implementation than the one in the main code, as here, we aren't concerned with the baseline comparisons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "4u1DjOVCP338"
   },
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "class ModulatedSineLayer(nn.Module):\n",
    "    def __init__(self, in_features, out_features, bias=True,\n",
    "                 is_first=False, omega_0=30, modulation=True):\n",
    "        super().__init__()\n",
    "        self.omega_0 = omega_0\n",
    "        self.is_first = is_first\n",
    "        self.in_features = in_features\n",
    "        self.linear = nn.Linear(in_features, out_features, bias=bias)\n",
    "        \n",
    "        # modulation パラメータを追加（学習可能）\n",
    "        self.modulation = nn.Parameter(torch.ones(out_features)) if modulation else None\n",
    "        \n",
    "        self.init_weights()\n",
    "    \n",
    "    def init_weights(self):\n",
    "        with torch.no_grad():\n",
    "            if self.is_first:\n",
    "                self.linear.weight.uniform_(-1 / self.in_features, \n",
    "                                             1 / self.in_features)\n",
    "            else:\n",
    "                self.linear.weight.uniform_(-np.sqrt(6 / self.in_features) / self.omega_0, \n",
    "                                             np.sqrt(6 / self.in_features) / self.omega_0)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        linear_output = self.linear(input)\n",
    "        \n",
    "        # modulationを適用\n",
    "        if self.modulation is not None:\n",
    "            linear_output = linear_output * self.modulation\n",
    "        \n",
    "        return torch.sin(self.omega_0 * linear_output)\n",
    "    \n",
    "    def forward_with_intermediate(self, input): \n",
    "        intermediate = self.omega_0 * self.linear(input)\n",
    "        \n",
    "        # modulationを適用\n",
    "        if self.modulation is not None:\n",
    "            intermediate = intermediate * self.modulation\n",
    "        \n",
    "        return torch.sin(intermediate), intermediate\n",
    "\n",
    "class ModulatedSiren(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features, hidden_layers, out_features, outermost_linear=False, \n",
    "                 omega=30, modulation=True):\n",
    "        super().__init__()\n",
    "        self.omega = omega\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.hidden_features = hidden_features\n",
    "        self.net = []\n",
    "        \n",
    "        # 最初の層はis_first=True\n",
    "        self.net.append(ModulatedSineLayer(in_features, hidden_features, \n",
    "                                           is_first=True, omega_0=omega, modulation=modulation))\n",
    "\n",
    "        # 中間層\n",
    "        for i in range(hidden_layers):\n",
    "            self.net.append(ModulatedSineLayer(hidden_features, hidden_features, \n",
    "                                               is_first=False, omega_0=omega, modulation=modulation))\n",
    "\n",
    "        # 最後の層\n",
    "        if outermost_linear:\n",
    "            final_linear = nn.Linear(hidden_features, out_features)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                final_linear.weight.uniform_(-np.sqrt(6 / hidden_features) / omega, \n",
    "                                              np.sqrt(6 / hidden_features) / omega)\n",
    "                \n",
    "            self.net.append(final_linear)\n",
    "        else:\n",
    "            self.net.append(ModulatedSineLayer(hidden_features, out_features, \n",
    "                                               is_first=False, omega_0=omega, modulation=modulation))\n",
    "        \n",
    "        self.net = nn.Sequential(*self.net)\n",
    "    \n",
    "    def forward(self, coords):\n",
    "        coords = coords.clone().detach().requires_grad_(True)  # allows to take derivative w.r.t. input\n",
    "        output = self.net(coords)\n",
    "        return output \n",
    "\n",
    "    def forward_with_activations(self, coords, retain_grad=False):\n",
    "        '''Returns not only model output, but also intermediate activations.\n",
    "        Only used for visualizing activations later!'''\n",
    "        activations = OrderedDict()\n",
    "\n",
    "        activation_count = 0\n",
    "        x = coords.clone().detach().requires_grad_(True)\n",
    "        activations['input'] = x\n",
    "        for i, layer in enumerate(self.net):\n",
    "            if isinstance(layer, ModulatedSineLayer):\n",
    "                x, intermed = layer.forward_with_intermediate(x)\n",
    "                \n",
    "                if retain_grad:\n",
    "                    x.retain_grad()\n",
    "                    intermed.retain_grad()\n",
    "                    \n",
    "                activations['_'.join((str(layer.__class__), \"%d\" % activation_count))] = intermed\n",
    "                activation_count += 1\n",
    "            else: \n",
    "                x = layer(x)\n",
    "                \n",
    "                if retain_grad:\n",
    "                    x.retain_grad()\n",
    "                    \n",
    "            activations['_'.join((str(layer.__class__), \"%d\" % activation_count))] = x\n",
    "            activation_count += 1\n",
    "\n",
    "        return activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def laplace(y, x):\n",
    "    grad = gradient(y, x)\n",
    "    return divergence(grad, x)\n",
    "\n",
    "\n",
    "def divergence(y, x):\n",
    "    div = 0.\n",
    "    for i in range(y.shape[-1]):\n",
    "        div += torch.autograd.grad(y[..., i], x, torch.ones_like(y[..., i]), create_graph=True)[0][..., i:i+1]\n",
    "    return div\n",
    "\n",
    "\n",
    "def gradient(y, x, grad_outputs=None):\n",
    "    if grad_outputs is None:\n",
    "        grad_outputs = torch.ones_like(y)\n",
    "    grad = torch.autograd.grad(y, [x], grad_outputs=grad_outputs, create_graph=True)[0]\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v5F38S8vP339"
   },
   "source": [
    "# Experiments\n",
    "\n",
    "For the image fitting and poisson experiments, we'll use the classic cameraman image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "v0hKZ00vP33-"
   },
   "outputs": [],
   "source": [
    "from scipy.io import wavfile\n",
    "class AudioFile(Dataset):\n",
    "    def __init__(self, filename):\n",
    "        super().__init__()\n",
    "        self.rate, self.data = wavfile.read(filename)  # 音声ファイルを読み込む\n",
    "        # ステレオ音声の場合はモノラルに変換\n",
    "        if len(self.data.shape) > 1 and self.data.shape[1] == 2:\n",
    "            self.data = np.mean(self.data, axis=1)\n",
    "        self.data = self.data.astype(np.float32)  # データをfloat32にキャスト\n",
    "        self.file_length = len(self.data)  # 音声ファイルのサンプル数\n",
    "        print(f\"Rate: {self.rate} Hz, Length: {self.file_length} samples\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return 1\n",
    "    \n",
    "    def get_length(self):\n",
    "        return self.file_length\n",
    "\n",
    "    def get_rate(self):\n",
    "        return self.rate\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.rate, self.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P3eSNo_XP33-"
   },
   "source": [
    "## Fitting an audio signal\n",
    "<a id='section_2'></a>\n",
    "\n",
    "Here, we'll use Siren to parameterize an audio signal - i.e., we seek to parameterize an audio waverform $f(t)$  at time points $t$ by a SIREN $\\Phi$.\n",
    "\n",
    "That is we seek the function $\\Phi$ such that:  $\\mathcal{L}\\int_\\Omega \\lVert \\Phi(t) - f(t) \\rVert \\mathrm{d}t$  is minimized, in which  $\\Omega$  is the domain of the waveform.\n",
    "\n",
    "For the audio, we'll use the bach sonata:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "aQzYm8ECP33-"
   },
   "outputs": [],
   "source": [
    "# ImplicitAudioWrapperクラス: 音声データを格納し、座標データを生成\n",
    "class ImplicitAudioWrapper(Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "        # -100から100までの範囲で、音声ファイルの長さに対応する均等なグリッドを生成\n",
    "        self.grid = np.linspace(start=-100, stop=100, num=dataset.file_length)\n",
    "        self.grid = self.grid.astype(np.float32)\n",
    "        self.grid = torch.Tensor(self.grid).view(-1, 1)  # グリッドをPyTorchのテンソルに変換\n",
    "\n",
    "    def get_num_samples(self):\n",
    "        return self.grid.shape[0]\n",
    "\n",
    "    def __len__(self):\n",
    "        return 1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # 音声データとサンプリングレートを取得\n",
    "        rate, data = self.dataset[idx]\n",
    "        # 音声データをスケールし、テンソルに変換\n",
    "        scale = np.max(np.abs(data))  # 音声データの最大値でスケール\n",
    "        data = (data / scale)  # 正規化\n",
    "        data = torch.Tensor(data).view(-1, 1)  # PyTorchテンソルに変換\n",
    "        # 座標テンソルと正規化された音声データを返す\n",
    "        return {'idx': idx, 'coords': self.grid}, {'func': data, 'rate': rate, 'scale': scale}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zqA0cjGaP33-",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let's build a little dataset that computes coordinates for audio files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mbv5uX6wP34A",
    "outputId": "c20b7e14-c7b2-4b1f-b60e-3d711dca4ff2",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# AudioFileオブジェクトを作成\n",
    "audio_dataset = AudioFile(filename=\"sirenvoice3.wav\")\n",
    "# ImplicitAudioWrapperを使用してデータをラップ\n",
    "coord_dataset = ImplicitAudioWrapper(audio_dataset)\n",
    "# DataLoaderを作成\n",
    "dataloader = DataLoader(coord_dataset, shuffle=True, batch_size=1, pin_memory=True, num_workers=0)\n",
    "audio_samples = audio_dataset.get_length()\n",
    "audio_rate = audio_dataset.get_rate()\n",
    "\n",
    "# データローダーからデータを取り出す\n",
    "for batch in dataloader:\n",
    "    coords = batch[0]['coords']\n",
    "    func = batch[1]['func']\n",
    "    rate = batch[1]['rate']\n",
    "    scale = batch[1]['scale']\n",
    "    print(f\"Coords: {coords.shape}, Audio Data: {func.shape}, Rate: {rate}, Scale: {scale}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bqfzs_rHP34A"
   },
   "source": [
    "We now fit Siren in a simple training loop. Within only hundreds of iterations, the image and its gradients are approximated well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 453
    },
    "id": "NUtj64QNf2aa",
    "outputId": "5c43f066-2a2e-4884-d0c2-c289bc2ede2f"
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def train_siren(dataloader, hidden_features, hidden_layers, omega, epochs):\n",
    "    \"\"\"\n",
    "    SIRENモデルをトレーニングする関数（音声データ用）\n",
    "\n",
    "    Args:\n",
    "        dataloader (DataLoader): 音声データ用のデータローダー\n",
    "        hidden_features (int): 隠れ層のユニット数\n",
    "        hidden_layers (int): 隠れ層の数\n",
    "        omega (float): 初期周波数\n",
    "        epochs (int): エポック数\n",
    "\n",
    "    Returns:\n",
    "        Siren: トレーニング済みのSIRENモデル\n",
    "    \"\"\"\n",
    "    # データローダーから最初のバッチを取得\n",
    "    batch = next(iter(dataloader))\n",
    "    \n",
    "    # バッチ次元を除去\n",
    "    model_input = batch[0]['coords'].squeeze(0)  # 形状 [N, 1]\n",
    "    ground_truth = batch[1]['func'].squeeze(0)   # 形状 [N, 1]\n",
    "    \n",
    "    # モデルをGPUに移動する場合は以下を有効化\n",
    "    model_input, ground_truth = model_input.cuda(), ground_truth.cuda()\n",
    "\n",
    "    best_loss = float('inf')\n",
    "    siren_best = None\n",
    "    optimizer_best = None\n",
    "    plot_epochs =  epochs / 5 #グラフを表示する頻度\n",
    "\n",
    "    for stage in range(2):\n",
    "        # ステージごとに異なる初期設定が可能\n",
    "        for epoch in range(epochs):\n",
    "            if stage == 0:\n",
    "                # 初期段階では1次元入力用にSIRENモデルを初期化\n",
    "                siren = ModulatedSiren(in_features=1, out_features=1, hidden_features=hidden_features, \n",
    "                              hidden_layers=hidden_layers, outermost_linear=True, omega=omega)\n",
    "                siren.cuda()  # GPUを使用する場合\n",
    "                optimizer = optim.Adam(siren.parameters(), lr=1e-4)\n",
    "            elif stage == 1:\n",
    "                # 2段階目では学習率を下げるなどの調整が可能\n",
    "                siren = siren_best\n",
    "                optimizer = optim.Adam(siren.parameters(), lr=1e-5)\n",
    "\n",
    "            # トレーニングモードに設定\n",
    "            siren.train()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # フォワードパス\n",
    "            model_output = siren(model_input)  # 出力テンソル（[N, 1]）\n",
    "\n",
    "            # ロス計算（MSE）\n",
    "            loss = nn.MSELoss()(model_output, ground_truth)\n",
    "\n",
    "            # バックワードパス\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # ロスの監視とモデルの保存\n",
    "            if loss.item() < best_loss:\n",
    "                best_loss = loss.item()\n",
    "                siren_best = copy.deepcopy(siren)\n",
    "                optimizer_best = copy.deepcopy(optimizer)\n",
    "            \n",
    "            # ログの出力\n",
    "            if (epoch + 1) % 10 == 0 or epoch == 0:\n",
    "                print(f\"Stage: {stage}, Epoch: {epoch+1}/{epochs}, Loss: {loss.item():.6f}\")\n",
    "\n",
    "                if (stage != 0 and (epoch+1) % plot_epochs == 0) or (stage == 0 and loss < best_loss):\n",
    "                    # モデルの出力を可視化\n",
    "                    siren.eval()\n",
    "                    with torch.no_grad():\n",
    "                        output = model_output.cpu().numpy()  # 形状 [N, 1]\n",
    "                    ground = ground_truth.cpu().numpy()           # 形状 [N, 1]\n",
    "\n",
    "                    # プロット時にバッチ次元を除去し、形状を調整\n",
    "                    plt.figure(figsize=(12, 6))\n",
    "                    plt.plot(batch[0]['coords'].squeeze(0).cpu().detach().numpy().flatten(), \n",
    "                            ground.flatten(), label='Ground Truth')\n",
    "                    plt.plot(batch[0]['coords'].squeeze(0).cpu().detach().numpy().flatten(), \n",
    "                            output.flatten(), label='SIREN Output')\n",
    "                    plt.legend()\n",
    "                    plt.title(f'Stage: {stage}, Epoch: {epoch+1}, Loss: {loss.item():.6f}')\n",
    "                    plt.xlabel('Time')\n",
    "                    plt.ylabel('Amplitude')\n",
    "                    plt.show()\n",
    "\n",
    "        # ステージ終了後、最良モデルを保持\n",
    "        siren = siren_best\n",
    "        optimizer = optimizer_best\n",
    "\n",
    "    siren_best.eval()\n",
    "    outplt = siren_best(model_input)\n",
    "    outplt = outplt.cpu().detach().numpy()\n",
    "\n",
    "    return siren_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Audio\n",
    "siren_voice = train_siren(dataloader, 28, 1, 20, 1000000) \n",
    "#siren_voice = train_siren(dataloader, 32, 1, 20, 9000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5noJ1lN7fCnj"
   },
   "source": [
    "Generate an audio waveform using the trained SIREN model and save it as a WAV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio\n",
    "def save_audio(model, dataloader, num_samples, sample_rate, path):\n",
    "    \"\"\"\n",
    "    学習済みSIRENモデルを使用してオーディオ波形を生成し、WAVファイルとして保存します。\n",
    "\n",
    "    Args:\n",
    "        model (Siren): 学習済みのSIRENモデル。\n",
    "        duration (float): オーディオの長さ（秒）。\n",
    "        sample_rate (int): サンプリングレート（例: 44100）。\n",
    "        path (str): 保存するWAVファイルのパス。\n",
    "    \"\"\"\n",
    "    model.eval() #モデルを評価状態に切り替える\n",
    "    with torch.no_grad():\n",
    "        # サンプル数の計算\n",
    "        duration = num_samples / sample_rate\n",
    "        # 0からdurationまでの線形間隔の時間座標を生成\n",
    "        t = torch.linspace(0, duration, steps=num_samples).unsqueeze(1)  # 形状 [num_samples, 1]\n",
    "        \n",
    "        # モデルがGPU上にある場合はデバイスを合わせる\n",
    "        device = next(model.parameters()).device\n",
    "        t = t.to(device)\n",
    "        \n",
    "        # データローダーから最初のバッチを取得\n",
    "        batch = next(iter(dataloader))\n",
    "        model_input = batch[0]['coords'].squeeze(0)  # 形状 [N, 1]\n",
    "        model_input = model_input.cuda()\n",
    "        outplt = model(model_input)\n",
    "        waveform = outplt.cpu().numpy()\n",
    "        print(f\"Waveform shape before processing: {waveform.shape}\")\n",
    "\n",
    "        plt.plot(batch[0]['coords'].squeeze(0).cpu().detach().numpy().flatten(), waveform, label='SIREN Output')\n",
    "        plt.legend()\n",
    "        plt.xlabel('Time')\n",
    "        plt.ylabel('Amplitude')\n",
    "        plt.show()\n",
    "        \n",
    "        # 正規化（-1から1の範囲にスケーリング）\n",
    "        max_val = np.max(np.abs(waveform))\n",
    "        if max_val > 1:\n",
    "            waveform = waveform / max_val\n",
    "            print(f\"maxval: {max_val}\")\n",
    "        \n",
    "        # float32型に変換\n",
    "        waveform = waveform.astype(np.float32)\n",
    "        \n",
    "        # 波形の形状を [1, samples] に変更\n",
    "        if waveform.ndim == 2 and waveform.shape[1] == 1:\n",
    "            waveform = waveform.T  # 形状を [1, samples] に転置\n",
    "            print(f\"Waveform shape after transposing: {waveform.shape}\")  # 例: (1, 28848)\n",
    "        elif waveform.ndim == 1:\n",
    "            waveform = np.expand_dims(waveform, axis=0)  # 形状を [1, samples] にする\n",
    "            print(f\"Waveform shape after expanding dims: {waveform.shape}\")\n",
    "        else:\n",
    "            raise ValueError(f\"Unexpected waveform shape: {waveform.shape}\")\n",
    "        \n",
    "        # 再度形状を確認（デバッグ用）\n",
    "        print(f\"Final waveform shape: {waveform.shape}\")\n",
    "        \n",
    "        # PyTorchテンソルに変換\n",
    "        waveform_tensor = torch.from_numpy(waveform)\n",
    "    \n",
    "    # torchaudioを使用してWAVファイルとして保存\n",
    "    try:\n",
    "        torchaudio.save(path, waveform_tensor, sample_rate)\n",
    "        print(f\"Audio saved successfully at {path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to save audio: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_audio(siren_voice, dataloader, audio_samples, audio_rate, \"siren_best.wav\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generates the shader code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def dump_data(dat):\n",
    "    dat = dat.cpu().detach().numpy()\n",
    "    return dat\n",
    "\n",
    "def print_vec4(ws):\n",
    "    vec = \"vec4(\" + \",\".join([\"{0:.4g}\".format(w) for w in ws]) + \")\"\n",
    "    vec = re.sub(r\"\\b0\\.\", \".\", vec)\n",
    "    return vec\n",
    "\n",
    "def print_mat4(ws):\n",
    "    mat = \"mat4(\" + \",\".join([\"{0:.3g}\".format(w) for w in np.transpose(ws).flatten()]) + \")\"\n",
    "    mat = re.sub(r\"\\b0\\.\", \".\", mat)\n",
    "    return mat\n",
    "\n",
    "def serialize_to_shadertoy(siren, varname):\n",
    "    omega = siren.omega\n",
    "    hidden_features = siren.hidden_features\n",
    "    hidden_layers = siren.hidden_layers\n",
    "    chunks = int(hidden_features / 4)\n",
    "    print(f\"//Chunksds: {chunks}, omega: {omega}, hidden_features: {hidden_features}, hidden_layers: {hidden_layers}\")\n",
    "    \n",
    "    # 最初の層の重み、バイアス、modulationを取得\n",
    "    in_w = dump_data(siren.net[0].linear.weight)  # shape: [hidden_features, in_features]\n",
    "    in_bias = dump_data(siren.net[0].linear.bias)  # shape: [hidden_features]\n",
    "    modulation = dump_data(siren.net[0].modulation)  # shape: [hidden_features]\n",
    "    \n",
    "    in_features = in_w.shape[1]\n",
    "    \n",
    "    # 最初の層の処理\n",
    "    for row in range(chunks):\n",
    "        if in_features == 2:\n",
    "            x_vec = in_w[row*4:(row+1)*4, 1] * omega\n",
    "            y_vec = in_w[row*4:(row+1)*4, 0] * -1 * omega\n",
    "            bias = in_bias[row*4:(row+1)*4] * omega\n",
    "            mod_vec = modulation[row*4:(row+1)*4]\n",
    "            shader_line = (\n",
    "                f\"vec4 {varname}0_{row} = sin((uv.x * {print_vec4(x_vec)} + \"\n",
    "                f\"uv.y * {print_vec4(y_vec)} + {print_vec4(bias)}) * {print_vec4(mod_vec)});\"\n",
    "            )\n",
    "        elif in_features == 1:\n",
    "            x_vec = in_w[row*4:(row+1)*4, 0] * omega\n",
    "            bias = in_bias[row*4:(row+1)*4] * omega\n",
    "            mod_vec = modulation[row*4:(row+1)*4]\n",
    "            shader_line = (\n",
    "                f\"vec4 {varname}0_{row} = sin((t * {print_vec4(x_vec)} + {print_vec4(bias)}) * {print_vec4(mod_vec)});\"\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported number of input features: {in_features}\")\n",
    "        \n",
    "        print(shader_line)\n",
    "    \n",
    "    # 隠れ層の処理\n",
    "    for layer in range(hidden_layers):\n",
    "        layer_w = dump_data(siren.net[layer+1].linear.weight)  # shape: [hidden_features, hidden_features]\n",
    "        layer_bias = dump_data(siren.net[layer+1].linear.bias)  # shape: [hidden_features]\n",
    "        modulation = dump_data(siren.net[layer+1].modulation)  # shape: [hidden_features]\n",
    "\n",
    "        for row in range(chunks):\n",
    "            line = f\"vec4 {varname}{layer+1}_{row} = sin(\\n\"\n",
    "            for col in range(chunks):\n",
    "                mat = layer_w[row*4:(row+1)*4, col*4:(col+1)*4] * omega\n",
    "                line += f\"    {print_mat4(mat)} * {varname}{layer}_{col} +\\n\"\n",
    "            bias = layer_bias[row*4:(row+1)*4] * omega\n",
    "            mod_vec = modulation[row*4:(row+1)*4]\n",
    "            line += f\"    {print_vec4(bias)} * {print_vec4(mod_vec)});\"\n",
    "            print(line)\n",
    "    \n",
    "    # 出力層の処理\n",
    "    out_w = dump_data(siren.net[-1].weight)  # shape: [1, hidden_features]\n",
    "    out_bias = dump_data(siren.net[-1].bias)  # shape: [1]\n",
    "    \n",
    "    line = f\"float {varname} = \"\n",
    "    for row in range(chunks):\n",
    "        vec = out_w[0, row*4:(row+1)*4]\n",
    "        line += f\"dot({varname}{hidden_layers}_{row}, {print_vec4(vec)}) +\\n    \"\n",
    "    line += f\"{out_bias[0]};\"\n",
    "    print(line)\n",
    "\n",
    "print(\"//luma network\")\n",
    "serialize_to_shadertoy(siren_voice, \"f\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "sad_cat.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
